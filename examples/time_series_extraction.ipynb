{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25e98353-f6f1-450f-b43b-084703ee4f74",
   "metadata": {},
   "source": [
    "# Time series extraction for a specific location\n",
    "\n",
    "This example demonstrates the extraction of time series data for the variable \"plant available water\"  \n",
    "from a depth of 13 meters. The data will be extracted for two locations  \n",
    "Location 1 in Jülich, latitude: 47.60170, longitude: 9.31862  \n",
    "Location 2 in Amsterdam, latitude 52.37677, longitude: 5.04244  \n",
    "The time series saved as an ASCII file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87c0ee9-4c98-4135-aca3-d7ec34e17554",
   "metadata": {},
   "source": [
    "First, we need to import all libraries needed in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c13cbe-e789-4597-9863-afd9e8d45d78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "import datetime\n",
    "import csv\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130de2c6-2f22-457a-9ab2-bc7529c49699",
   "metadata": {},
   "source": [
    "The following function calculates the spherical distance between two points.  \n",
    "It will help identify the nearest neighbor from the simulation grids to the specified location (Jülich in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb1f63c1-0af2-405a-9c50-49478a7bb854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def spher_dist( lon1, lat1, lon2, lat2, Rearth=6371):\n",
    "\n",
    "    term1 = np.sin(lat1) * np.sin(lat2)\n",
    "    term2 = np.cos(lat1) * np.cos(lat2)\n",
    "    term3 = np.cos(lon2 - lon1)\n",
    "    \n",
    "    return Rearth * np.arccos(term1+term2*term3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2947202f-9fba-4333-ac75-d6191b0df33a",
   "metadata": {},
   "source": [
    "The following function find the lower boundary of the layer  where the inserted depth is within."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad07c3f2-8131-4e8e-ac05-97259c32ed77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_depth_index(depth):\n",
    "    \n",
    "    depth_list = [60.0, 42.0, 27.0, 17.0, 7.0, 3.0, 2.0, 1.3, 0.8, 0.5, 0.3,\n",
    "                  0.17, 0.1, 0.05, 0.02]\n",
    "\n",
    "   #if the depth is the lower boundary of the layer\n",
    "    try:\n",
    "        index = depth_list.index(depth)\n",
    "        print(index)\n",
    "        return index\n",
    "\n",
    "    #if the depth is within a layer, find the layer boundaries which\n",
    "    #the depth falls in between\n",
    "    \n",
    "    except ValueError:\n",
    "\n",
    "        lower_boundary = None\n",
    "        higher_boundary = None\n",
    "\n",
    "        for i in range(len(depth_list) - 1):\n",
    "            if depth_list[i] >= depth > depth_list[i + 1]:\n",
    "                higher_boundary = depth_list[i + 1]\n",
    "                lower_boundary = depth_list[i]\n",
    "                depth_n = lower_boundary\n",
    "                break\n",
    "        index_n = depth_list.index(depth_n)\n",
    "\n",
    "        return index_n,depth_n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738abf18-f333-4b35-b1df-656949dc4462",
   "metadata": {},
   "source": [
    "First we have to read in some data we can apply the data extraction. For this example we take some of the tool example data provided under  \n",
    "the Data/ directory. In particular, data_input_example_1.json and DE-0055_INDICATOR_regridded_rescaled_SoilGrids250-v2017_BGRvector_newAllv.nc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06707dd6-d89e-4731-8407-9653ca5f609d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_input = '/p/project/pfgpude05/hammoudeh1/ParflowCLM_postpro_scripts/data_extraction/data_input_v1.json'\n",
    "lls_indicators = '/p/project/pfgpude05/hammoudeh1/ParflowCLM_postpro_scripts/data_extraction/DE-0055_INDICATOR_regridded_rescaled_SoilGrids250-v2017_BGRvector_newAllv.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45ed8b28-47af-404a-8cf9-0c5e9a9a7260",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location: Jülich\n",
      "Found:\n",
      "Mapped x index: 637\n",
      "Mapped y index: 1113\n",
      "data for Lat:50.92258, Lon:6.36205 is extracted and being written in a CSV file\n",
      "============\n",
      "csv file saved\n",
      " \n",
      "Location: Amsterdam\n",
      "Found:\n",
      "Mapped x index: 535\n",
      "Mapped y index: 1397\n",
      "The chosen location is located directly on a lake, river or sea, please insert a different location.\n"
     ]
    }
   ],
   "source": [
    "with open(data_input, 'r') as f:\n",
    "    locations = json.load(f)\n",
    "    for location in locations:\n",
    "        stationID = location[\"stationID\"]\n",
    "        stationLat = location[\"stationLat\"]\n",
    "        stationLon = location[\"stationLon\"]\n",
    "        fncdata = location[\"ParFlowData\"]\n",
    "        depth = location['Depth']\n",
    "        print(f'Location: {stationID}')\n",
    "\n",
    "        #finding the index of the Lon and Lat in interest\n",
    "        with Dataset(fncdata, 'r') as nc:\n",
    "\n",
    "            SimLons=nc.variables['lon'][:]\n",
    "            SimLats=nc.variables['lat'][:]\n",
    "\n",
    "        dist = spher_dist(np.deg2rad(SimLons),np.deg2rad(SimLats),np.deg2rad(stationLon),np.deg2rad(stationLat)) \n",
    "\n",
    "        mapped_idx = np.unravel_index(np.argmin(dist, axis=None), dist.shape)\n",
    "        MapYIdx = mapped_idx[0]\n",
    "        MapXIdx = mapped_idx[1]\n",
    "\n",
    "        print(f'Found:')\n",
    "        print(f'Mapped x index: {MapXIdx}')\n",
    "        print(f'Mapped y index: {MapYIdx}')\n",
    "\n",
    "        #extract the infromation for the soil layers and determine whether \n",
    "        #the point in interest is located directly in a water body such as rivers, lakes or seas\n",
    "        with Dataset(lls_indicators, 'r') as ncIndicator: \n",
    "           indicator_value = ncIndicator.variables['Indicator'][0,14,MapYIdx,MapXIdx]\n",
    "\n",
    "        # Check if the location falls dierectly on a river, lake or sea\n",
    "        if indicator_value in [19,20, 21]:\n",
    "\n",
    "        # Find the indices that would sort the distances array in ascending order\n",
    "          sorted_indices = np.argsort(dist, axis=None)\n",
    "\n",
    "        # Convert the 1D indices to 2D indices\n",
    "          sorted_indices_2d = np.unravel_index(sorted_indices, dist.shape)\n",
    "\n",
    "        #Get the first 9 indices, which correspond to the closest distances\n",
    "          closest_indices_9 = list(zip(sorted_indices_2d[0][1:10], sorted_indices_2d[1][1:10]))\n",
    "          lons_to_add = []\n",
    "          lats_to_add = []\n",
    "        # test if the other nearest neighbours are on a river, lake, sea\n",
    "          for i in range(len(closest_indices_9)):\n",
    "              with Dataset(lls_indicators, 'r') as ncIndicator:\n",
    "                   indicator_value = ncIndicator.variables['Indicator'][0,14,sorted_indices_2d[0][i],sorted_indices_2d[1][i]]\n",
    "\n",
    "                   if indicator_value not in [19,20,21]:\n",
    "                      lon_n9 = ncIndicator.variables['lon'][sorted_indices_2d[0][i],sorted_indices_2d[1][i]]\n",
    "                      lon_n9 = lon_n9.item()\n",
    "                      lons_to_add.append(lon_n9)\n",
    "                      lat_n9 = ncIndicator.variables['lat'][sorted_indices_2d[0][i],sorted_indices_2d[1][i]]\n",
    "                      lat_n9 = lat_n9.item()\n",
    "                      lats_to_add.append(lat_n9)\n",
    "          if not lons_to_add:\n",
    "              print(\"The chosen location is located directly on a lake, river or sea, please insert a different location.\")\n",
    "              break\n",
    "\n",
    "          print(\"The chosen location is located directly on a lake, river or sea, here are other suggested location/locations:\")\n",
    "          for n in range(len(lats_to_add)):\n",
    "                latitude = \"{:.5f}\".format(lats_to_add[n])\n",
    "                longitude = \"{:.5f}\".format(lons_to_add[n])\n",
    "                print(f\"Latitude: {latitude}, Longitude: {longitude}\")\n",
    "                break\n",
    "\n",
    "        # if the condition passed, the next line will find the lowest boundary \n",
    "        # of the depth in question\n",
    "        else:\n",
    "            depth_index,depth_n = find_depth_index(depth)\n",
    "\n",
    "        # extract the data of the variable from the netcdf file\n",
    "            with Dataset(fncdata, 'r') as nc:\n",
    "                variables = nc.variables.keys()\n",
    "                variable = list(variables)[-1]\n",
    "                var=nc.variables[variable][:,depth_index,MapYIdx,MapXIdx]\n",
    "                unit = nc.variables[variable]\n",
    "                unit = unit.units\n",
    "                variable_long_name = nc.variables[variable].long_name\n",
    "                dates = nc.variables['time'][:]\n",
    "            print(f'data for Lat:{stationLat}, Lon:{stationLon} is extracted and being written in a CSV file')\n",
    "            startDate_hr = dates[0]\n",
    "            reference_startDate = datetime.datetime(1901, 1, 1)\n",
    "            readable_startDate = reference_startDate + datetime.timedelta(hours=float(startDate_hr))\n",
    "            startDate = readable_startDate.strftime(\"%Y%m%d\")\n",
    "\n",
    "            endDate_hr = dates[-1]\n",
    "            reference_endDate = datetime.datetime(1901, 1, 1)\n",
    "            readable_endDate = reference_endDate + datetime.timedelta(hours=float(endDate_hr))\n",
    "            endDate =readable_endDate.strftime(\"%Y%m%d\")\n",
    "\n",
    "            #open an CSV and save the time series\n",
    "            nameCSV = f'Station_{stationID[0]}_ADAPTER_DE05_ECMWF-HRES-forecast_FZJ-IBG3-ParFlowCLM380_v04aJuwelsGpuProd_{variable}_{depth_n}m-depth_{startDate}-{endDate}.csv'\n",
    "            today_date = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "            fCSV = open(nameCSV, 'w', newline='')\n",
    "            writer = csv.writer(fCSV)\n",
    "            writer.writerow(['stationID:',f'{stationID[0]}'])\n",
    "            writer.writerow(['stationLat:',f'{stationLat}'])\n",
    "            writer.writerow(['stationLon:',f'{stationLon}'])\n",
    "            writer.writerow(['Parameter:',variable_long_name])\n",
    "            writer.writerow(['Unit:',unit])\n",
    "            writer.writerow(['Time aggregation:','daily'])\n",
    "            writer.writerow(['Depth:',f'{depth_n} m'])\n",
    "            writer.writerow(['Model simulation:','ParFlow/CLM DE05_domain 611m resolution'])\n",
    "            writer.writerow(['Atmospheric forcing:','ECMWF HRES daily deterministic forecast 0-24h'])\n",
    "            writer.writerow(['Institution:','Forschungszentrum Juelich IBG-3'])\n",
    "            writer.writerow(['Time series extracted on:', today_date])\n",
    "            writer.writerow([''])\n",
    "\n",
    "            #dayDate = startDate\n",
    "            dayDate = datetime.datetime.strptime(startDate,\"%Y%m%d\")\n",
    "            dayDate = dayDate.replace(hour=12, minute=0, second=0)\n",
    "\n",
    "            for dd in range(len(dates)):\n",
    "                date_value = dates[dd]\n",
    "                reference_date = datetime.datetime(1901, 1, 1)\n",
    "                readable_date = reference_date + datetime.timedelta(hours=float(date_value))\n",
    "                dayDate = readable_date.replace(hour=12, minute=0, second=0)\n",
    "                var1Drow = [f'{dayDate}']\n",
    "\n",
    "                var1Drow.append(var[dd])\n",
    "\n",
    "                writer.writerow(var1Drow)\n",
    "\n",
    "\n",
    "            fCSV.close()\n",
    "            print('============')\n",
    "            print('csv file saved')\n",
    "            print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b188ed-10bb-4f16-866b-0f8ae98f81b1",
   "metadata": {},
   "source": [
    "An example of how the CSV will look like is provided under the name Station_Jülich_ADAPTER_DE05_ECMWF-HRES-forecast_FZJ-IBG3-ParFlowCLM380_v04aJuwelsGpuProd_paw_17.0m-depth_20230101-20231231.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_notbook",
   "language": "python",
   "name": "jupyter_notbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
